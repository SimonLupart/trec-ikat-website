{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to TREC Interactive Knowledge Assistance Track","text":"<p>Last updated: June 29, 2023</p> <p>Voice-based assistant interactions are now widespread, with a recent Comscore report showing that over 20% of homes in America own a smart speaker. Furthermore, the recent announcement of assistant-enabled smart glasses from leading tech companies continues this trend towards real-world interaction. However, despite current assistants' ability to perform well-defined simple actions, their ability to support information seeking in conversations continues to be limited.</p> <p>Conversational Information Seeking (CIS) is an established and important research direction. It is of interest to the broad research community within information retrieval, such as ranking, summarizing and question answering (QA), as well as for natural language processing and dialogue systems communities.</p> <p>The TREC Interactive Knowledge Assistance Track (iKAT) builds on the experience of four successful years of TREC Conversational Assistance Track (CAsT), where the key focus of iKAT is on researching and developing collaborative information seeking conversational agents which can tailor and personalize their response based on what they learn about and from the user.</p> <p>The fourth year of CAsT aimed to add more conversational elements to the interaction streams, by introducing mixed initiatives (clarifications, and suggestions) to create multi-path, multi-turn conversations for each topic. TREC iKAT evolves CAsT into a new track to signal this new trajectory. iKAT aims to focus on supporting multi-path, multi-turn, multi-perspective conversations, i.e., for a given topic, the direction and the conversation that evolves depends not only on the prior responses but also on the user (and their background/perspective/context/etc). As different personas undertake various topics, systems need to build and develop a picture of who the user is, in order to best address their information needs. Put another way, iKAT focuses on system understanding of user knowledge and information needs in accordance to the available context.</p> <p>Shield: </p> <p>All data associated with this work is licensed and released under a Creative Commons Attribution-ShareAlike 4.0 International License.</p> <p></p>"},{"location":"#track-coordinators","title":"Track Coordinators","text":"<p>Mohammad Aliannejadi, University of Amsterdam, The Netherlands. Dr. Aliannejadi is an Assistant Professor at the IRLab (formerly known as ILPS), the University of Amsterdam in The Netherlands. His research is in modeling user information needs with a focus on recommender systems, unified (meta) search, and conversational systems. </p> <p>Zahra Abbasiantaeb, University of Amsterdam, The Netherlands. Zahra is a first year Ph.D. student at the IRLab supervised by Dr. Aliannejadi. She is working on conversational search and recommendation. Earlier, she has also worked on patent reference mining. Zahra obtained her masters in AI from the Amirkabir University of Technology with a focus on Question Answering systems.</p> <p>Shubham Chatterjee, University of Glasgow, Scotland. Dr. Chatterjee is a Research Associate in the Glasgow Representation and Information Learning (GRILL) Lab, part of the Glasgow Information Retrieval group. The goal of his research is to design intelligent search systems which would one day respond to a user's open-ended and complex information needs with a complete answer instead of a ranked list of results.</p> <p>Jeff Dalton, University of Glasgow, Scotland. Dr. Dalton is a Senior Lecturer (Associate Professor) at the Department of Computing Science, University of Glasgow. He is also the PI for the GRILL Lab. His research focuses on new methods for machine understanding of language and text data using deep neural networks and entity knowledge graphs for improving information seeking applications.</p> <p>Leif Azzopardi, University of Strathclyde, Scotland. Dr. Azzopardi is an Associate Professor in Artifical Intelligence and Data Science within the Departement of Computer and Information Sciences at the University of Strathclyde. He is the PI for the Interaction Lab (i-lab) which specializes in developing, evaluating and modelling information rich and information intensive applications and agents.</p>"},{"location":"#contact","title":"Contact","text":"<ul> <li>Twitter: @trec_ikat</li> <li>Email: trec.ikat.ai@gmail.com</li> <li>Google Groups: trec-ikat@googlegroups.com</li> <li>Slack: ikat-2023</li> </ul>"},{"location":"#announcements","title":"Announcements","text":"<ul> <li> <p> New! Train topics released! See the data section of the website. </p> </li> <li> <p>Sample topics released! See the data section of the website. </p> </li> <li> <p>Guidelines released!</p> </li> </ul>"},{"location":"data/","title":"Datasets and Resources","text":""},{"location":"data/#sample-topic","title":"Sample Topic","text":"<p>We are releasing two sample conversations on the topic \"Finding a University\". Each conversation pertains to a different persona, and the conversation is personalized to that persona. You can find the sample topic here.</p>"},{"location":"data/#topics-for-ikat-year-1","title":"Topics for iKAT Year 1","text":"File Description 2023_train_topics.json Train topics in JSON format. Contains 11 conversations spanning 8 topics."},{"location":"data/#additional-data-from-trec-cast","title":"Additional Data from TREC CAsT","text":"<p>Those who are eager to start developing and experimenting can also use the data from previous years' TREC CAsT. The iKAT topics are going to be similar, with the addition of the Personal Text Knowledge Base. For more information on TREC CAsT, see the website and read the overview papers [2019] [2020] [2021] [2022]</p> <p>Note. TREC CAsT did not include a PTKB but you can be creative and modify the data according to your needs. Also, TREC CAsT used different collections (Wikipedia, KILT, MS MARCO, etc.) at different stages. iKAT will be using a subset of the recently released ClueWeb22-B.</p> <p>Below, we provide links to the TREC CAsT topics from previous years for participants. </p>"},{"location":"data/#cast-year-4-2022","title":"CAsT Year 4 (2022)","text":"File Description 2022_automatic_evaluation_topics_tree_v1.0.json Contains each conversation tree (topic) with an automatic rewrite generated for each user utterance. 2022_evaluation_topics_turn_ids.json Contains each conversation tree (topic) with the resolved query for each user utterance. 2022_evaluation_topics_tree_v1.0.json Contains all ids that responses/ranked passages need to be returned for. 2022_evaluation_topics_flattened_duplicated_v1.0.json Contains all possible conversation paths across all the conversation trees."},{"location":"data/#cast-year-3-2021","title":"CAsT Year 3 (2021)","text":"File Description 2021_automatic_evaluation_topics_v1.0.json 25 primary evaluation topics in JSON format. Variant: Automatic 2021_manual_evaluation_topics_v1.0.json 25 primary evaluation topics in JSON format. Variant: Manual 2021qrels.txt Qrels file for passage ranking task."},{"location":"data/#cast-year-2-2020","title":"CAsT Year 2 (2020)","text":"File Description 2020_automatic_evaluation_topics_v1.0.json 25 primary evaluation topics in JSON format. Variant: Automatic 2020_manual_evaluation_topics_v1.0.json 25 primary evaluation topics in JSON format. Variant: Manual 2020qrels.txt Qrels file for passage ranking task."},{"location":"data/#cast-year-1-2019","title":"CAsT Year 1 (2019)","text":"File Description train_topics_v1.0.json 30 example training topics in JSON format. evaluation_topics_v1.0.json 50 evaluation topics in JSON format. 2019qrels.txt Official evaluation qrels file for passage ranking task. train_qrels.txt Limited (incomplete) training judegements for 5 topics (approximately 50 turns). The judgments are graded on a three point scale (2 very relevant, 1 relevant, and 0 not relevant)."},{"location":"guidelines/","title":"Guidelines for the task","text":"<p>The guidelines are also available as a Google Doc.</p>"},{"location":"guidelines/#participation","title":"Participation","text":"<p>Participants must register to submit. To request a late registration, please email trec@nist.gov requesting a registration key. The dissemination form must be returned to submit runs.</p>"},{"location":"guidelines/#track-overview","title":"Track Overview","text":"<p>In iKAT, the direction of the conversation can be changed in each turn based on:</p> <ol> <li> <p>The previous response from the user, and\u00a0</p> </li> <li> <p>The information learned from the user (background, perspective, and context).\u00a0</p> </li> </ol> <p>The persona of the user and their information needs form the direction of the conversation. Each topic will have multiple  conversations based on multiple personas and results in different outputs that demonstrates the personalized aspect of the conversations. To this aim, the persona and the information needs of the user are modeled by generating a Personal Textual Knowledge Base (PTKB) during the conversation.\u00a0</p> <p>Note: In the first year, the PTKB is provided for each conversation and the participants do not have to generate or update it.</p>"},{"location":"guidelines/#task-overview","title":"Task Overview","text":"<p>In Year 1, the input at each conversation turn that is provided to the participants is the following:\u00a0</p> <ol> <li> <p>Current user's information need</p> </li> <li> <p>Conversation history</p> </li> <li> <p>Personal Text Knowledge Base (PTKB).</p> </li> </ol> <p>We offer the following tasks:</p> <ul> <li> <p>Passage Ranking: Retrieve and rank relevant passages from the given collection in response to a user utterance.\u00a0</p> </li> <li> <p>Response Generation: For each turn, return a series of text responses. Each response may be simply a passage from the collection. Alternatively, it may also be an extracted or generated summary from one or more passage results. All responses must have at least one passage called \"provenance\" from the collection.</p> </li> <li> <p>PTKB Statement Ranking: The relevant statements from PTKB are to be determined by the system in each turn. We approach this task as a relevance score prediction task. So, this output is in the form of a sorted list of the statements from PTKB with corresponding relevance score.</p> </li> </ul> <p>We plan on providing baseline ranking and response generation methods.\u00a0</p>"},{"location":"guidelines/#submission-classes","title":"Submission Classes","text":"<ul> <li> <p>Automatic: These use raw utterances (with automatic rewrite/expansion) methods.</p> </li> <li> <p>Manual: These use the manual human rewritten queries.</p> </li> </ul>"},{"location":"guidelines/#example-dialogue-tree","title":"Example Dialogue Tree","text":"<p>An example of two different conversations based on different personas for the same topic is shown in the following figure. For each user turn, systems should return a ranked list of text responses. Each response has one or more (ranked) source passages as provenance. In addition, the systems should provide a sorted list of relevant statements of PTKB with the corresponding relevance score.</p> <p></p> <p>For an explanation of the above diagram, see the Google Doc.</p>"},{"location":"guidelines/#primary-task-details","title":"Primary Task Details","text":"<p>The main task in iKAT can be defined as personalized retrieval-based \"candidate response retrieval\" in context of the conversation. The task can be divided into the following sub-tasks:</p> <ul> <li> <p>Read the current dialogue turns up to the given turn (context). The provided context is:\u00a0(1) A fixed set of previous responses with provenance in the preceding turns up to the current step, and (2) PTKB of the user. Note: Using information from following turns is not allowed.</p> </li> <li> <p>Find the relevant statements from PTKB to the information needed for this turn. This task is considered as a relevance score prediction. The output is in the form of a sorted list of the statements from PTKB with corresponding relevance score.</p> </li> <li> <p>Extract or generate a response. Each response can be generated from multiple passages. It can be an abstractive or extractive summary of the corresponding passages. Each response must have one or more ranked passages as provenance used to produce it.</p> </li> </ul>"},{"location":"guidelines/#what-is-a-response","title":"What is a response?","text":"<ul> <li> <p>A response is a text suitable for showing to the user. It should be fluent, satisfy their information needs, and not contain extraneous or redundant information.\u00a0</p> </li> <li> <p>A response is limited to a maximum of 250 words (as measured by the <code>Tokenizer</code> function of <code>spacy.tokenizer</code> in spaCy v3.3 library), but should vary depending on an appropriate query-response.</p> </li> </ul>"},{"location":"guidelines/#passage-provenance-ranking","title":"Passage (provenance) Ranking","text":"<ul> <li> <p>A \"run\" takes the provenance passages for all responses in response order. The first 1000 provenances for each turn will be ranked.\u00a0</p> </li> <li> <p>Because a response may have multiple source passages, the score of passages in the provenance list for a response is used to order passages in descending order.</p> </li> <li> <p>If a source passage occurs in multiple responses, it will be ranked by its first response.\u00a0</p> </li> <li> <p>Each provenance is written in the format doc_id:passage_id.</p> </li> </ul>"},{"location":"guidelines/#ptkb-statement-provenance-ranking","title":"PTKB Statement (provenance) Ranking","text":"<ul> <li> <p>A 'run' takes the provenance PTKB statements for all responses on which the system's response is based.</p> </li> <li> <p>Because a response may consider multiple PTKB statements, the score of statements in the provenance list for a response is used to order statements in descending order.</p> </li> <li> <p>Irrelevant statements can get the relevance score of '0' or not appear in the list.</p> </li> </ul>"},{"location":"guidelines/#collection","title":"Collection","text":"<p>The text collection contains a subset of ClueWeb22-B documents, prepared by the organizers in collaboration with CMU. The goal is to retrieve passages from target open-domain text collections. Provenance must be provided from documents in the collection.\u00a0</p> <p> Note. Each participating team must sign an individual agreement and return it to the organizers to be able to download the collection. The individual agreement can be found here.</p>"},{"location":"guidelines/#passage-segmentation","title":"Passage Segmentation","text":"<p>For assessment, we will judge provenance passages. We segment the documents in our collection into passages in a similar manner as done by the TREC Deep Learning track for segmenting MS MARCO documents into passages: First, each document is trimmed to 10k characters. Then a 10-sentence sliding window with a 5-sentence stride is used to generate the passages.\u00a0</p> <p>An example document with some passage segmentation is provided in TrecWeb format below for illustration purposes:</p> <p></p>"},{"location":"guidelines/#topic-format","title":"Topic Format","text":"<p>We will provide several sample topics with example baseline runs for validation and testing. Below is a sample topics file with two subtrees of the same topic. Subtrees are identified by topic and subtree ID,  i.e topic 1, subtree 2 is <code>1-2</code>. Also a <code>passage_provenance</code> field with a list of provenance passages and <code>ptkb_provenance</code> field with a list of provenance statements from PTKB, that are used for generating the response, are included. An example is shown below for illustrative purposes. </p> <pre><code>[\n  {\n    \"number\": \"1-1\",\n        \"title\": \"Finding a university\",\n        \"ptkb\": {\n            \"1\": \"I graduated from Tilburg university.\",\n            \"2\": \"I live in the Netherlands.\",\n            \"3\": \"I'm allergic to peanuts.\",\n            \"4\": \"I worked as a web developer for 2 years.\",\n            \"5\": \"I have a bachelor's degree in computer science.\",\n            \"6\": \"I like Indian food.\",\n            \"7\": \"My bachelor's GPA is 5.6.\",\n            \"8\": \"I'm 26 years old.\",\n            \"9\": \"My TOEFL SCORE is 91.\",\n            \"10\": \"My interesting bachelor courses are data structure, algorithm, data mining, and artificial intelligence.\",\n            \"11\": \"I didn't like computer architecture and logical circuits courses.\"\n        },\n        \"turns\": [\n            {\n                \"turn_id\": 1,\n                \"utterance\": \"I want to start my master's degree, can you help me with finding a university?\",\n                \"resolved_utterance\": \"I want to start my master's degree, can you help me with finding a university?\",\n                \"response\": \"Do you want to continue your bachelor's studies and obtain a degree in computer science?\",\n                \"ptkb_provenance\": [\n                    5\n                ],\n                \"response_provenance\": []\n            },\n            {\n                \"turn_id\": 2,\n                \"utterance\": \"Yes, I want to continue my studies in computer science.\",\n                \"resolved_utterance\": \"Yes, I want to continue my studies in computer science.\",\n                \"response\": \"Do you want to study in the Netherlands, Europe, or somewhere further away?\",\n                \"ptkb_provenance\": [\n                    2\n                ],\n                \"response_provenance\": []\n            },\n            {\n                \"turn_id\": 3,\n                \"utterance\": \"I'd like to stay here.\",\n                \"resolved_utterance\": \"I'd like to stay in the Netherlands.\",\n                \"response\": \"I can help you with finding a university for continuing your studies in the Netherlands as a computer science student. Take a look at these Top Computer Science Universities in the Netherlands: Delft University of Technology, Eindhoven University of Technology, Vrije Universiteit Amsterdam, University of Amsterdam, Leiden University, Radboud University, Utrecht University, University of Twente\",\n                \"ptkb_provenance\": [\n                    5,\n                    2\n                ],\n                \"response_provenance\": [\n                    \"clueweb22-en0034-09-03452:1\"\n                ]\n            },\n  }\n  {\n        \"number\": \"1-2\",\n        \"title\": \"Finding a university\",\n        \"ptkb\": {\n            \"1\": \"I don't like crazy cold weather.\",\n            \"2\": \"I don't have a driver's license.\",\n            \"3\": \"I plan to move to Canada.\",\n            \"4\": \"I'm from the Netherlands.\",\n            \"5\": \"I'm used to heavy rains in the Netherlands.\",\n            \"6\": \"I graduated from UvA.\",\n            \"7\": \"I have bachelor's degree in computer science.\",\n            \"8\": \"I speak English fluently.\"\n        },\n        \"turns\": [\n            {\n                \"turn_id\": 1,\n                \"utterance\": \"I want to start my master's degree, can you help me with finding a university?\",\n                \"resolved_utterance\": \"I want to start my master's degree, can you help me with finding a university in Canada?\",\n                \"response\": \"Sure, do you want to study computer science?\",\n                \"ptkb_provenance\": [\n                    7,\n                    3\n                ],\n                \"response_provenance\": []\n            },\n            {\n                \"turn_id\": 2,\n                \"utterance\": \"Yes, I want to pursue the same major. Can you tell me the name of the best universities?\",\n                \"resolved_utterance\": \"Yes, I want to pursue computer science. Can you tell me the name of the best computer science universities in Canada?\",\n                \"response\": \"Here are the top universities for computer science in Canada: 1) University of British Columbia, 2) University of Alberta, 3)Concordia University, 4) Simon Fraser University, 5)The University of Toronto\",\n                \"ptkb_provenance\": [],\n                \"response_provenance\": [\n                    \"clueweb22-en0026-31-15538:1\",\n                    \"clueweb22-en0026-31-15538:4\",\n                    \"clueweb22-en0026-31-15538:6\",\n                    \"clueweb22-en0040-41-06056:0\"\n                ]\n            },\n            {\n                \"turn_id\": 3,\n                \"utterance\": \"Which of them best suits me in terms of weather conditions?\",\n                \"resolved_utterance\": \"Which of the following universities best suits me in terms of weather conditions? 1) the University of British Columbia, 2) the University of Alberta, 3)Concordia University, 4) Simon Fraser University, and 5)The University of Toronto.\",\n                \"response\": \"I know you don't like very cold weather, but can you give me an estimation of the temperature that is acceptable for you?\",\n                \"ptkb_provenance\": [\n                    1,\n                    5\n                ],\n                \"response_provenance\": []\n            },\n  }\n]\n\n\n\n\n</code></pre>"},{"location":"guidelines/#task-submissions","title":"Task Submissions","text":"<p>Participants submit the output of their system on the specified \u201ctest\u201d topics.  A single participant may submit the output of multiple systems, up to a maximum of four runs. A sample run is provided below:</p> <pre><code>{\n  \"run_name\": \"sample_run\",\n  \"run_type\": \"automatic\",\n  \"turns\": [\n    {\n      \"turn_id\": \"1-2_3\",\n      \"responses\": [\n        {\n          \"rank\": 1,\n          \"text\": \"The University of British columbia in Vancouver has temperatures near 80 degrees Fahrenheit (27 degrees Celsius) in summer and up to 45 degrees Fahrenheit (about \ndegrees Celsius) in winter which is suitable for you. The university of Toronto is acceptable since has cold winters, average temperatures can drop below -10 \u00b0 C but not below 12 degrees for long. The Concordia university in Montreal is not suitable for you since in the winter, could reach minus 40 with the wind chill. University of Alberta is also not suitable for you. In winter the average temperature varies between -6.5\u00b0C (20.3\u00b0F) and -13.5\u00b0C (7.7\u00b0F). Simon Fraser university is not acceptable for you. The city which the university is located in will reach temperatures of -14 in the winter.\",\n          \"ptkb_provenance\": [\n            {\n              \"id\": \"1\",\n              \"text\": \"I cannot withstand the temperature below -12 for long time\",\n              \"score\": 0.9\n            },\n            {\n              \"id\": \"2\",\n              \"text\": \"I\u2019m used to heavy rains in the Netherlands\",\n              \"score\": 0.8\n            }\n          ],\n          \"passage_provenance\": [\n            {\n              \"id\": \"clueweb22-en0000-94-02275:0\",\n              \"text\": \"...\",\n              \"score\": 0.6\n            },\n            {\n              \"id\": \"clueweb22-en0027-06-08704:1\",\n              \"text\": \"...\",\n              \"score\": 0.5\n            },\n            {\n              \"id\": \"clueweb22-en0005-63-12144:0\",\n              \"text\": \"...\",\n              \"score\": 0.4\n            },\n            {\n              \"id\": \"clueweb22-en0013-01-17558:1\",\n              \"text\": \"...\",\n              \"score\": 0.38\n            },\n            {\n              \"id\": \"clueweb22-en0014-39-04143:0\",\n             \"text\": \"...\",\n              \"score\": 0.3\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n</code></pre> <ol> <li>The run_name is a run submission identifier that should be descriptive and unique to your team and institution.</li> <li>The run_type is one of the two types listed above, automatic and manual.</li> <li>Each turn in the turns list should contain a turn_identifier, consisting of the topic_id-subtree_id and turn_id concatenated with an underscore, e.g. <code>1-2_3</code> for topic 1, subtree 2, turn 3.</li> <li>Each turn should also contain a list of responses. A response consists of text and a provenance list. Each provenance should have an id, text, and score.</li> <li>Each turn includes a sorted list of statements from PTKB based on the relevance score of each statement from PTKB to the current turn.</li> </ol> <p>For provenance ranking, this will be converted to a traditional TREC run format:</p> <p><code>31_1-1 Q0 clueweb22-en0000-94-02275:0 1 0.5 sample_run</code></p> <p>Runs may include up to 1000 responses for each user turn. For provenance ranking, only the first 1000 pieces of unique provenance will be used. As in previous years of CAsT, only a limited top-K responses and provenances will be assessed according to resource constraints. </p>"},{"location":"guidelines/#evaluation","title":"Evaluation","text":"<p>We will use the relevance assessment methods used in previous years of CAsT for relevance to individual turns.</p> <ol> <li>Provenance Passage Assessment: The provenance passages that are used to produce the responses will be pooled and assessed. The relevance scale will be the same as previous years of CAsT, see the previous overview papers for details. The standard ranking metrics such as P@k, NDCG@k, and MAP will be calculated using the judgments. We will focus on the earlier positions (1, 3, 5).</li> <li>Response Assessment: A response may be a simple passage or a summary of one or more passages in providing a response. We will assess the top ranked response (or top-k) from all systems for all turns. Only responses with at least one relevant provenance passage will be judged. Responses will be assessed for relevance and conciseness. The responses and judgments on them will be released with the judgments. </li> <li>Extracted Relevant Statements Assessment: The standard metrics like Precision, Recall, P@k, and MAP will be used for evaluating the sorted list of relevant statements from PTKB.</li> </ol> <p>Similar to CAsT year 4, only a subset of turns may be evaluated for provenance ranking effectiveness. This will be disclosed to participants after assessment is completed.</p>"},{"location":"guidelines/#timeline","title":"Timeline","text":"Task Date Guidelines released April 2023 Test topics released June 2023 Submission deadline August 2023 Results released to participants October 2023"}]}